{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaLDxEgc1K5tC6wrzIuCO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jared-Steven/YouTube-Video-Transcriber/blob/main/YouTube_Video_Transcriber_With_an_AI_Speech_To_Text_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing The Libraries Needed For The Task\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zkUN34YQlS5s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lOHEVXDv6yr"
      },
      "outputs": [],
      "source": [
        "!pip install whisper\n",
        "!pip install moviepy\n",
        "!pip install pydub\n",
        "!pip install transformers\n",
        "!pip install gradio==3.43.1\n",
        "!pip install pytube\n",
        "!pip install noisereduce\n",
        "!pip install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing The Huggingface Transformer For Trascribing The Audio From The Video"
      ],
      "metadata": {
        "id": "u7OVeB8OlTWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git accelerate datasets[audio]"
      ],
      "metadata": {
        "id": "m-bZypiCk4Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import The Needed Libraries"
      ],
      "metadata": {
        "id": "f6yvg7tNlT7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "from moviepy.editor import VideoFileClip\n",
        "import noisereduce as nr\n",
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import warnings\n",
        "import gradio as gr\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "X5jKXtzck7-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below block of code is written to download the youtube video and extract the audio from it and save it in the local drive."
      ],
      "metadata": {
        "id": "39qQVrjHlVG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the YouTube video\n",
        "def download_video(video_url, output_path):\n",
        "    yt = YouTube(video_url)\n",
        "    stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
        "    stream.download(output_path)\n",
        "    return stream.default_filename\n",
        "\n",
        "# Extract audio from the video\n",
        "def extract_audio(video_file, output_path):\n",
        "    video = VideoFileClip(video_file)\n",
        "    audio = video.audio\n",
        "    audio.write_audiofile(f\"{output_path}/audio.mp3\")\n",
        "    audio.close()\n",
        "    video.close()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    video_url = \"https://www.youtube.com/watch?v=Sby1uJ_NFIY\"\n",
        "    output_path = \"/content\"\n",
        "\n",
        "    video_file = download_video(video_url, output_path)\n",
        "    extract_audio(f\"{output_path}/{video_file}\", output_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fZ2fY9tulBfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buildin a speech recognition pipeline using a pre-trained model from Hugging Face\n",
        "\n",
        "Step 1: Device and Data Type Selection\n",
        "\n",
        "    Device Selection (device):\n",
        "        Check if a CUDA-enabled GPU is available using torch.cuda.is_available().\n",
        "        If a GPU is available, device is set to \"cuda:0\", which means the first GPU device.\n",
        "        If no GPU is available, device is set to \"cpu\", meaning the code will run on the CPU.\n",
        "\n",
        "    Data Type Selection (torch_dtype):\n",
        "        If a GPU is available, torch_dtype is set to torch.float16 for half-precision floating-point, which is faster and uses less memory on GPUs.\n",
        "        If no GPU is available, torch_dtype is set to torch.float32, which is the default precision for CPUs.\n",
        "\n",
        "Step 2: Model Loading\n",
        "\n",
        "    Model Identifier (model_id):\n",
        "        The identifier for the pre-trained model is set to \"washeed/audio-transcribe\". This string is typically the name or path of the model in a model repository like Hugging Face's Model Hub.\n",
        "\n",
        "    Loading the Model:\n",
        "        AutoModelForSpeechSeq2Seq.from_pretrained is a method from the Hugging Face Transformers library. It loads a pre-trained sequence-to-sequence model for speech recognition.\n",
        "        torch_dtype=torch_dtype specifies the data type to use for the model's tensors (either float16 or float32).\n",
        "        low_cpu_mem_usage=True minimizes the CPU memory usage during model loading.\n",
        "        use_safetensors=True uses a safer serialization format for model weights, improving security.\n",
        "        model.to(device) moves the model to the specified device (\"cuda:0\" or \"cpu\").\n",
        "\n",
        "Step 3: Processor Initialization\n",
        "\n",
        "    AutoProcessor.from_pretrained(model_id) loads a pre-trained processor associated with the model. The processor typically includes components like tokenizers and feature extractors necessary for preparing input data for the model.\n",
        "\n",
        "Step 4: Pipeline Creation\n",
        "\n",
        "    pipeline is a high-level API from the Hugging Face Transformers library that simplifies the process of using models for specific tasks.\n",
        "    Parameters:\n",
        "        \"automatic-speech-recognition\": Specifies the type of task the pipeline will perform.\n",
        "        model=model: Uses the previously loaded model.\n",
        "        tokenizer=processor.tokenizer: Uses the tokenizer from the processor to convert text to tokens.\n",
        "        feature_extractor=processor.feature_extractor: Uses the feature extractor from the processor to process audio input.\n",
        "        max_new_tokens=128: Sets the maximum number of new tokens the model will generate.\n",
        "        chunk_length_s=15: Sets the chunk length of the audio in seconds. The model processes audio in chunks of 15 seconds.\n",
        "        batch_size=16: Sets the batch size for processing the audio data.\n",
        "        return_timestamps=True: Configures the pipeline to return timestamps for the recognized speech.\n",
        "        torch_dtype=torch_dtype: Specifies the data type for the pipeline (either float16 or float32).\n",
        "        device=device: Specifies the device to run the pipeline on (\"cuda:0\" or \"cpu\")."
      ],
      "metadata": {
        "id": "-W7cEDt5lV-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"washeed/audio-transcribe\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    chunk_length_s=14,\n",
        "    batch_size=16,\n",
        "    return_timestamps=True,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")\n"
      ],
      "metadata": {
        "id": "JHojZFRDlBCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In Summary\n",
        "\n",
        "The above code sets up a speech recognition pipeline using a pre-trained model from Hugging Face.\n",
        "\n",
        "It selects the appropriate device and data type based on GPU availability, loads the model and processor, and configures the pipeline for automatic speech recognition with specified parameters for chunking, batch size, and output format."
      ],
      "metadata": {
        "id": "nyebNzHJlW1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting The Audio Into 14 Second Chuncks for accurate transcription.   "
      ],
      "metadata": {
        "id": "mzlgGZZcmG7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the path to your input mp3 file\n",
        "input_file = \"/content/audio.mp3\"\n",
        "\n",
        "# Replace with the desired output folder path\n",
        "output_folder = \"/content/drive/MyDrive/Audio Chunks\"\n",
        "\n",
        "# Load the input mp3 file\n",
        "audio = AudioSegment.from_mp3(input_file)\n",
        "\n",
        "# Calculate the total duration of the audio in milliseconds\n",
        "total_duration = len(audio)\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Split the audio into 14-second chunks\n",
        "for i, start_time in enumerate(range(0, total_duration, 14000)):\n",
        "    end_time = start_time + 14000\n",
        "    chunk = audio[start_time:end_time]\n",
        "\n",
        "    # Save the chunk as a new mp3 file\n",
        "    output_file = os.path.join(output_folder, f\"chunk_{i}.mp3\")\n",
        "    chunk.export(output_file, format=\"mp3\")\n",
        "    print(f\"Exported chunk {i} to {output_file}\")"
      ],
      "metadata": {
        "id": "azhtfZiKlA8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking if the number of files in the folder are accurate"
      ],
      "metadata": {
        "id": "IwOhQToqlXqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of all files and directories\n",
        "directory_path = '/content/drive/MyDrive/Audio Chunks'\n",
        "files_and_dirs = os.listdir(directory_path)\n",
        "\n",
        "# Filter out directories, keep only files\n",
        "files = [f for f in files_and_dirs if os.path.isfile(os.path.join(directory_path, f))]\n",
        "\n",
        "print(\"No of files in the folder:\", len(files))"
      ],
      "metadata": {
        "id": "Pw-2GG2YlA2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcribing The audio chunks and maping the audio and it's transcript  in a json format."
      ],
      "metadata": {
        "id": "DKBtMpQRlYf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert milliseconds to a formatted string (MM:SS)\n",
        "def format_time(milliseconds):\n",
        "    seconds = (milliseconds // 1000) % 60\n",
        "    minutes = (milliseconds // (1000 * 60)) % 60\n",
        "    return f\"{minutes:02}:{seconds:02}\"\n",
        "\n",
        "# Load the mp3 file\n",
        "audio = AudioSegment.from_mp3(\"/content/audio.mp3\")\n",
        "\n",
        "# Get the total duration of the audio in milliseconds\n",
        "total_duration = len(audio)\n",
        "print(f\"Total duration: {format_time(total_duration)}\")\n",
        "\n",
        "# Segment the audio into 14-second chunks\n",
        "chunk_duration = 14 * 1000  # 14 seconds in milliseconds\n",
        "time_stamp = []\n",
        "\n",
        "for start_time in range(0, total_duration, chunk_duration):\n",
        "    end_time = min(start_time + chunk_duration, total_duration)\n",
        "    time_stamp.append((start_time, end_time))\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for i in range(len(files)):\n",
        "  result = pipe(f\"/content/drive/MyDrive/Audio Chunks/chunk_{i}\"+\".mp3\", generate_kwargs={\"task\": \"transcribe\"})\n",
        "  dictio = {\n",
        "        \"chunk_id\": i,\n",
        "        \"chunk_length\": 14.0,\n",
        "        \"text\": result[\"text\"],\n",
        "        \"start_time\": format_time(time_stamp[i][0]),\n",
        "        \"end_time\": format_time(time_stamp[i][1])\n",
        "    }\n",
        "  chunks.append(dictio)\n",
        "\n",
        "print(' ')\n",
        "print('output: ')\n",
        "print(chunks)"
      ],
      "metadata": {
        "id": "WHNnrG9ElAwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elT3JljdlN8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glSRDCEnlNu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gtoCHyTplNcj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}